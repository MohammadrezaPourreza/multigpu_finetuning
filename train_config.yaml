model_id: "mistralai/Codestral-22B-v0.1" #Hugging Face model id
max_seq_length:  2048 # 2048              # max sequence length for model and packing of the dataset
train_dataset_path: "MrezaPRZ/synthetic_sql_queries_sample" # path to where SageMaker saves train dataset
hub_path: "MrezaPRZ/cdestral_synthetic_data_sqlite" # path to where SageMaker saves hub dataset
output_dir: "./NL2SQL"            # path to where SageMaker will upload the model 
sample_size: 10
# training parameters
hub_token: "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"
report_to: "tensorboard"               # report metrics to tensorboard
learning_rate: 0.0001                  # learning rate 2e-4
lr_scheduler_type: "cosine"          # learning rate scheduler
num_train_epochs: 1                   # number of training epochs
overwrite_output_dir: true             # overwrite output directory
per_device_train_batch_size: 1       # batch size per device during training
per_device_eval_batch_size: 1          # batch size for evaluation
gradient_accumulation_steps: 64         # number of steps before performing a backward/update pass
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 50                      # log every 10 steps
weight_decay: 0.01                     # weight decay
save_strategy: epoch                   # save checkpoint every epoch
eval_strategy: steps             # evaluate every epoch
max_grad_norm: 0.3                     # max gradient norm
warmup_ratio: 0.01                     # warmup ratio
bf16: false                             # use bfloat16 precision
tf32: false                             # use tf32 precision
gradient_checkpointing: true           # use gradient checkpointing to save memory
save_total_limit: 1                    # save only the last checkpoint
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: "full_shard auto_wrap" # remove offload if enough GPU memory
fsdp_config:
  backward_prefetch: "backward_pre"
  forward_prefetch: "false"
  use_orig_params: "false"